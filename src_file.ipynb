{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8ZGULmFMkLv"
      },
      "source": [
        "Import necessary Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d5lxwBRDMecT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from collections import defaultdict  \n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrKaoDqVWjrb"
      },
      "source": [
        "Information Gain Generator class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HKz4DThuWsEW"
      },
      "outputs": [],
      "source": [
        "class InformationGain:\n",
        "\n",
        "    \n",
        "    def calc_entropy(self,column):\n",
        "        \"\"\"\n",
        "        Calculate entropy given a pandas series, list, or numpy array.\n",
        "        \"\"\"\n",
        "        # Compute the counts of each unique value in the column\n",
        "        counts = np.bincount(column)\n",
        "        # Divide by the total column length to get a probability\n",
        "        probabilities = counts / len(column)\n",
        "        \n",
        "        # Initialize the entropy to 0\n",
        "        entropy = 0\n",
        "        # Loop through the probabilities, and add each one to the total entropy\n",
        "        for prob in probabilities:\n",
        "            if prob > 0:\n",
        "                # use log from math and set base to 2\n",
        "                entropy += prob * math.log(prob, 2)\n",
        "        \n",
        "        return -entropy\n",
        "\n",
        "    \n",
        "    def calc_information_gain(self,data, split_name, target_name):\n",
        "        \"\"\"\n",
        "        Calculate information gain given a data set, column to split on, and target\n",
        "        \"\"\"\n",
        "        # Calculate the original entropy\n",
        "        original_entropy = self.calc_entropy(data[target_name])\n",
        "        \n",
        "        #Find the unique values in the column\n",
        "        values = data[split_name].unique()\n",
        "        \n",
        "        \n",
        "        # Make two subsets of the data, based on the unique values\n",
        "        left_split = data[data[split_name] == values[0]]\n",
        "        right_split = data[data[split_name] == values[1]]\n",
        "        \n",
        "        # Loop through the splits and calculate the subset entropies\n",
        "        to_subtract = 0\n",
        "        for subset in [left_split, right_split]:\n",
        "            prob = (subset.shape[0] / data.shape[0]) \n",
        "            to_subtract += prob * self.calc_entropy(subset[target_name])\n",
        "        \n",
        "        # Return information gain\n",
        "        return original_entropy - to_subtract\n",
        "\n",
        "          \n",
        "    def highest_info_gain(self,dataset,columns,target):\n",
        "        #Intialize an empty dictionary for information gains\n",
        "        information_gains = {}\n",
        "        \n",
        "        #Iterate through each column name in our list\n",
        "        for col in columns:\n",
        "          #Find the information gain for the column\n",
        "          information_gain = self.calc_information_gain(dataset, col, target)\n",
        "          #Add the information gain to our dictionary using the column name as the ekey                                         \n",
        "          information_gains[col] = information_gain\n",
        "        \n",
        "        #Return the key with the highest value  \n",
        "        #print(information_gains) \n",
        "        list = []\n",
        "        for i in range(len(columns)) :\n",
        "\n",
        "              temp = max(information_gains, key=information_gains.get)\n",
        "              list.append(temp)\n",
        "              information_gains.pop(temp)\n",
        "\n",
        "        return list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5b05kcvbsmR"
      },
      "source": [
        "Processing Telecom Churn Prediction Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RFz9_0Bdb2Lw"
      },
      "outputs": [],
      "source": [
        "def object_to_int(dataframe_series):\n",
        "    if dataframe_series.dtype=='object':\n",
        "        dataframe_series = LabelEncoder().fit_transform(dataframe_series)\n",
        "    return dataframe_series\n",
        "\n",
        "def featurePriorityForTelcoData():\n",
        "    df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "    df = df.drop(['customerID'], axis = 1)\n",
        "    df['TotalCharges'] = pd.to_numeric(df.TotalCharges, errors='coerce')\n",
        "    df.drop(labels=df[df['tenure'] == 0].index, axis=0, inplace=True)\n",
        "    df.fillna(df[\"TotalCharges\"].mean())\n",
        "    df = df.apply(lambda x: object_to_int(x))\n",
        "    columns =df.columns[:-1]\n",
        "    infoGain = InformationGain()\n",
        "    featurePriority = infoGain.highest_info_gain(df,columns,'Churn')\n",
        "    return featurePriority\n",
        "\n",
        "def process_telecom_data(feature):\n",
        "    telco_featurePriority=featurePriorityForTelcoData()\n",
        "\n",
        "    telco_data = pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\") # column sort according to these priority\n",
        "    telco_data.dtypes\n",
        "\n",
        "    #treatment of missing value and process the data\n",
        "    telco_data['TotalCharges'] = pd.to_numeric(telco_data.TotalCharges, errors='coerce')\n",
        "    telco_data.drop(labels=telco_data[telco_data['tenure'] == 0].index, axis=0, inplace=True)\n",
        "    telco_data.fillna(telco_data[\"TotalCharges\"].mean())\n",
        "\n",
        "    feature_used = telco_featurePriority[0:feature]\n",
        "    feature_used.append('Churn')\n",
        "    telco_data = telco_data[feature_used]\n",
        "    #Convertin the predictor variable in a binary numeric variable\n",
        "    telco_data['Churn'].replace(to_replace='Yes', value=1, inplace=True)\n",
        "    telco_data['Churn'].replace(to_replace='No',  value=0, inplace=True)\n",
        "    #Let's convert all the categorical variables into dummy variables\n",
        "    df_dummies = pd.get_dummies(telco_data)\n",
        "\n",
        "    # We will use the data frame where we had created dummy variables\n",
        "    y = df_dummies['Churn'].values\n",
        "    X = df_dummies.drop(columns = ['Churn'])\n",
        "\n",
        "    # Scaling all the variables to a range of 0 to 1\n",
        "    features = X.columns.values\n",
        "    scaler = MinMaxScaler(feature_range = (0,1))\n",
        "    scaler.fit(X)\n",
        "    X = pd.DataFrame(scaler.transform(X))\n",
        "    X.columns = features\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XOSOEds9omD"
      },
      "source": [
        "Processing adult dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OA3gb7Be9x7R"
      },
      "outputs": [],
      "source": [
        "# def object_to_int(dataframe_series):\n",
        "#     if dataframe_series.dtype=='object':\n",
        "#         dataframe_series = LabelEncoder().fit_transform(dataframe_series)\n",
        "#     return dataframe_series\n",
        "\n",
        "def featurePriorityForAudultData():\n",
        "      'this function calculate feature priority for audult data'\n",
        "      missing_values = [\"n/a\", \"na\", \"--\",\"NA\",\"N/A\",\"?\"]\n",
        "      df = pd.read_csv('adult.data', header=None, na_values = missing_values, skip_blank_lines=True)\n",
        "      df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation',\n",
        "                    'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'salary']\n",
        "\n",
        "      # checking whether it has missing value or not\n",
        "      df.isnull().sum()\n",
        "      # no missing value found \n",
        "      df = df.apply(lambda x: object_to_int(x))\n",
        "      columns =df.columns[:-1] #exclude last column which have to be claculated\n",
        "      infoGain = InformationGain()\n",
        "      featurePriority = infoGain.highest_info_gain(df,columns,'salary')\n",
        "      return featurePriority\n",
        "\n",
        "def process_audult_data(feature):\n",
        "      'audult data test train spliter'  \n",
        "      audult_featurePriority=featurePriorityForAudultData()\n",
        "      missing_values = [\"n/a\", \"na\", \"--\",\"NA\",\"N/A\",\"?\"]\n",
        "      audult_data =pd.read_csv('adult.data', header=None, na_values = missing_values, skip_blank_lines=True)\n",
        "      audult_data.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation',\n",
        "                          'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'salary']\n",
        "      audult_data.dtypes \n",
        "      #checking whether it has missing value or not\n",
        "      audult_data.isnull().sum()\n",
        "      #no missing value\n",
        "\n",
        "\n",
        "      feature_used = audult_featurePriority[0:feature]\n",
        "      feature_used.append( 'salary')\n",
        "      audult_data = audult_data[feature_used]\n",
        "      #Convertin the predictor variable in a binary numeric variable\n",
        "      audult_data['salary'].replace(to_replace=' >50K', value=1, inplace=True)\n",
        "      audult_data['salary'].replace(to_replace=' <=50K', value=0, inplace=True)\n",
        "      audult_data.dtypes\n",
        "      audult_data.head()\n",
        "      #Let's convert all the categorical variables into dummy variables\n",
        "      df_dummies = pd.get_dummies(audult_data)\n",
        "      df_dummies.head()\n",
        "\n",
        "      # # We will use the data frame where we had created dummy variables\n",
        "      y = df_dummies['salary'].values\n",
        "      X = df_dummies.drop(columns = ['salary'])\n",
        "\n",
        "\n",
        "      # Scaling all the variables to a range of 0 to 1\n",
        "      features = X.columns.values\n",
        "      scaler = MinMaxScaler(feature_range = (0,1))\n",
        "      scaler.fit(X)\n",
        "      X = pd.DataFrame(scaler.transform(X))\n",
        "      X.columns = features\n",
        "      X.head()\n",
        "      return train_test_split(X, y, test_size=0.2, random_state=101)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing CreditCardFraud Dataset"
      ],
      "metadata": {
        "id": "iQCPngbn9FUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def object_to_int(dataframe_series):\n",
        "#     if dataframe_series.dtype=='object':\n",
        "#         dataframe_series = LabelEncoder().fit_transform(dataframe_series)\n",
        "#     return dataframe_series\n",
        "\n",
        "def featurePriorityForCreditData():\n",
        "    'this function calculate feature priority for CreditCardFraud data'\n",
        "    missing_values = [\"n/a\", \"na\", \"--\",\"NA\",\"N/A\",\"?\"]\n",
        "    df = pd.read_csv('creditcard.csv', na_values = missing_values, skip_blank_lines=True)\n",
        "    pos_sample = df[df.iloc[:,-1] == 1]\n",
        "    neg_sample = df[df.iloc[:,-1] == 0].sample(n=20000, random_state=0)\n",
        "    df = shuffle(pd.concat( (pos_sample,neg_sample), axis=0), random_state=0) \n",
        "    df.reset_index(drop=True)\n",
        "    df.head()\n",
        "    #checking data type\n",
        "    df.dtypes\n",
        "    # checking whether it has missing value or not\n",
        "    df.isnull().sum()\n",
        "    # no missing value found \n",
        "    df = df.apply(lambda x: object_to_int(x))\n",
        "    columns =df.columns[:-1] #exclude last column which have to be claculated\n",
        "    infoGain = InformationGain()\n",
        "    featurePriority = infoGain.highest_info_gain(df,columns,'Class')\n",
        "    return featurePriority\n",
        "\n",
        "\n",
        "\n",
        "def process_credit_data(feature):\n",
        "    'CreditCardFraud data test train spliter'  \n",
        "    credit_featurePriority=featurePriorityForCreditData()\n",
        "    missing_values = [\"n/a\", \"na\", \"--\",\"NA\",\"N/A\",\"?\"]\n",
        "    credit_data =pd.read_csv('creditcard.csv', na_values = missing_values, skip_blank_lines=True)\n",
        "    pos_sample = credit_data[credit_data.iloc[:,-1] == 1]\n",
        "    neg_sample = credit_data[credit_data.iloc[:,-1] == 0].sample(n=20000, random_state=0)\n",
        "    credit_data = shuffle(pd.concat( (pos_sample,neg_sample), axis=0), random_state=0) \n",
        "    credit_data.reset_index(drop=True)\n",
        "    credit_data.head()\n",
        "    #checking whether it has missing value or not\n",
        "    credit_data.isnull().sum()\n",
        "    #no missing value\n",
        "\n",
        "    feature_used = credit_featurePriority[0:feature]\n",
        "    feature_used.append( 'Class')\n",
        "    credit_data = credit_data[feature_used]\n",
        "\n",
        "    #Let's convert all the categorical variables into dummy variables\n",
        "    df_dummies = pd.get_dummies(credit_data)\n",
        "    df_dummies.head()\n",
        "\n",
        "    # # We will use the data frame where we had created dummy variables\n",
        "    y = df_dummies['Class'].values\n",
        "    X = df_dummies.drop(columns = ['Class'])\n",
        "\n",
        "\n",
        "    # Scaling all the variables to a range of 0 to 1\n",
        "    features = X.columns.values\n",
        "    scaler = MinMaxScaler(feature_range = (0,1))\n",
        "    scaler.fit(X)\n",
        "    X = pd.DataFrame(scaler.transform(X))\n",
        "    X.columns = features\n",
        "    X.head()\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=101)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l7tXCp6l9MMT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0t1aqVVWEZF"
      },
      "source": [
        "Code for Logistic regression class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Nj2XnqBjWIm3"
      },
      "outputs": [],
      "source": [
        "class LogitRegression():\n",
        "          def __init__(self,learning_rate , iterations ):\n",
        "              self.learning_rate = learning_rate\n",
        "              self.iterations = iterations\n",
        "\n",
        "          def fit ( self, X , Y ) :\n",
        "              self.m , self.n = X.shape\n",
        "              self.W = np.zeros ( self.n )\n",
        "              self.b = 0\n",
        "              self.X = X\n",
        "              self.Y = Y\n",
        "\n",
        "              #gradient descent learning\n",
        "              for i in range( self.iterations) :\n",
        "                  self.update_weights()\n",
        "              return self\n",
        "\n",
        "          def update_weights(self) :\n",
        "              A = ( np.exp ( ( self.X.dot (self.W ) + self.b ))  - np.exp ( - ( self.X.dot (self.W ) + self.b )) ) / ( np.exp ( ( self.X.dot (self.W ) + self.b ))  + np.exp ( - ( self.X.dot (self.W ) + self.b )) )\n",
        "              #res = np.identity(len(A)) - (np.dot(A,A.T)) #line1\n",
        "              tmp = ( A - self.Y.T)\n",
        "              tmp = np.reshape( tmp , self.m )\n",
        "              #tmp = np.dot(res,tmp) #line2\n",
        "              dW = np.dot ( self.X.T , tmp ) / self.m\n",
        "              db = np.sum(tmp ) / self.m\n",
        "              self.W = self.W - self.learning_rate * dW\n",
        "              self.b = self.b - self.learning_rate * db\n",
        "              return self \n",
        "\n",
        "          def predict ( self , X ) :\n",
        "      \n",
        "              Z =( np.exp ( ( X.dot (self.W ) + self.b ))  - np.exp ( - ( X.dot (self.W ) + self.b )) ) / ( np.exp ( ( X.dot (self.W ) + self.b ))  + np.exp ( - ( X.dot (self.W ) + self.b )) )\n",
        "            \n",
        "              ans = np.where(Z> 0.5 , 1 , 0)\n",
        "             \n",
        "              return ans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTdIfknoYF-z"
      },
      "source": [
        "Code for Adaboost class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Rcxrf-KPbYNF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Adaboost:\n",
        "    \n",
        "    def __init__(self, numOfHypothesis):\n",
        "        self.hypoNum = numOfHypothesis\n",
        "        self.hypoList = [] \n",
        "        self.hypoWeight = [1] * numOfHypothesis\n",
        "\n",
        "    def normalize(self, lst):\n",
        "        lst = np.array(lst)\n",
        "        sum = np.sum(lst)\n",
        "        return lst/sum\n",
        "\n",
        "    def getResampleData(self):\n",
        "        chosenIdxs = random.choices( population= list( range( len(self.dataset) ) ), weights=self.sampleWeight, k=len(self.dataset) )\n",
        "        return self.dataset[chosenIdxs]\n",
        "\n",
        "    def fit(self, x_train, y_train):\n",
        "        x_train = np.array(x_train, dtype=\"float\")\n",
        "        y_train = np.array(y_train,  dtype=\"float\")\n",
        "        self.dataset = np.column_stack((x_train, y_train))\n",
        "        self.sampleWeight = [1/len(self.dataset)] * len(self.dataset)\n",
        "\n",
        "        k = 0\n",
        "        while k < self.hypoNum:\n",
        "            #dtc = DecisionTree(1)\n",
        "            dtc = LogitRegression( learning_rate = 0.1 , iterations = 1000)\n",
        "            data = self.getResampleData()\n",
        "\n",
        "            # dtc = DecisionTreeClassifier(random_state=0, max_depth=1)\n",
        "            # data = self.dataset\n",
        "            tmpHypoResult = self.hypothesisResult( dtc, data )\n",
        "            #tmpHypoResult = dtc.fit()\n",
        "            error = self.getError(tmpHypoResult, y_train)\n",
        "\n",
        "            if error > 0.5:\n",
        "                continue\n",
        "            for j in range(len(y_train)):\n",
        "                if tmpHypoResult[j] == y_train[j]:\n",
        "                    self.sampleWeight[j] = self.sampleWeight[j] * error/(1 - error)\n",
        "            \n",
        "            self.sampleWeight = self.normalize(self.sampleWeight)\n",
        "            self.hypoList.append(dtc)\n",
        "            self.hypoWeight[k] = math.log2( (1-error)/ error )\n",
        "            k = k + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def getError(self, y_pred, y_test):\n",
        "        error = 0\n",
        "        for i in range(len(y_pred)):\n",
        "            if y_pred[i] != y_test[i]:\n",
        "                error = error + self.sampleWeight[i]\n",
        "        return error\n",
        "\n",
        "    def hypothesisResult(self, dtc, resampleData ):\n",
        "        dtc.fit(resampleData[:,:-1], resampleData[:,-1] )\n",
        "\n",
        "        return dtc.predict(self.dataset[:,:-1])\n",
        " \n",
        "\n",
        "    def predict(self, x_test):\n",
        "        y_predLst = []\n",
        "        y_test = []\n",
        "        for hypo in self.hypoList:\n",
        "            y_test.append( hypo.predict(x_test) ) \n",
        "            \n",
        "        y_test = np.array(y_test)\n",
        "        for i in range(len(x_test)):\n",
        "            y_pred = y_test[:,i]\n",
        "            voting = defaultdict(int)\n",
        "            for k in range( self.hypoNum ):\n",
        "                voting[ y_pred[k] ] += self.hypoWeight[k]\n",
        "            y_predLst.append( max(voting, key=voting.get) ) \n",
        "        \n",
        "        return y_predLst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcY_eHEnceZb"
      },
      "source": [
        "Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAPU407zYE5l",
        "outputId": "d5cd02b2-031a-4489-f222-2b72319c36d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Data fitting start\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # input for telecom data\n",
        "    #X_train, X_test, y_train, y_test = process_telecom_data(18)\n",
        "    # input for audult data\n",
        "    #X_train, X_test, y_train, y_test = process_audult_data(8)\n",
        "    # input for credit data\n",
        "    X_train, X_test, y_train, y_test = process_credit_data(8)\n",
        "\n",
        "    for stump in [5,10,15,20]:\n",
        "      boost = Adaboost(stump)\n",
        "\n",
        "      start_time = time.time()\n",
        "      print('-- Data fitting start')\n",
        "\n",
        "      # boost.fit(x_train, y_train)\n",
        "      boost.fit(X_train, y_train)\n",
        "\n",
        "      print('Data fitting finished')\n",
        "      print('Time taken ----', time.time() - start_time)\n",
        "\n",
        "      print('Data prediction start')\n",
        "      start_time = time.time()\n",
        "\n",
        "      y_pred = boost.predict(X_test)\n",
        "      y_train_pred = boost.predict(X_train)\n",
        "      print('Data fitting finished')\n",
        "      print('Time taken ----', time.time() - start_time)\n",
        "\n",
        "      print('No of stumps ',stump)\n",
        "      print('Training set performance')\n",
        "      print(metrics.accuracy_score(y_train,y_train_pred) * 100)\n",
        "\n",
        "      print('Testing set performance')\n",
        "      print(metrics.accuracy_score(y_test,y_pred) * 100)\n",
        "      print('Data prediction finished')\n",
        "      print('Time taken ----', time.time() - start_time)\n",
        "      print('------------------------------------------')\n",
        "   \n",
        "main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ML_assignment_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}